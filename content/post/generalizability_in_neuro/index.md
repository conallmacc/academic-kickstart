---
title: 'The generalizability crisis goes pandemic.'
subtitle: ''
summary: Social psychology had a replication and generalizability crisis. Does neuroscience have its head in the sand?
authors:
- Conall E Mac Cionnaith
tags:
- Science
- Metascience
- Neuroscience
categories:
- Neuroscience
date: "2020-04-22T00:00:00Z"
lastmod: "2020-04-23T00:00:00Z"
featured: true
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Placement options: 1 = Full column width, 2 = Out-set, 3 = Screen-width
# Focal point options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
image:
  placement: 1
  caption: ''
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---
I’ve learned way more about rigorous methodology and good research practices from social psychologists than I have from neuroscientists. And, that's coming from someone who's spent their whole graduate career in a neurobiology department. Why? What are social psychologists doing that we are not?

Social psychologists are willing to admit when they have problems. First the replication crisis, fueled by [questionable (even nefarious) research practices](https://journals.sagepub.com/doi/full/10.1177/0956797611430953?casa_token=BR6ZgX-7jqAAAAAA%3AjaveqjINQrurLDjUJbIbt9CGwkI_wTD54qG5TBXCmKIpDWzchP3bP1w-f0M93_6USPwP8YXd6M5ZqQ) hit them. Textbook effects either disappeared or shriveled away to nothing, once the field rolled out some large scale [replication projects](https://science.sciencemag.org/content/349/6251/aac4716?casa_token=Gqed9LzVad8AAAAA:F3YfdI9B5sBhhVx4229xKceAInQtoNwUz7bcs2WBEqrUrGaQQkQCU0IsaZ-Wxaq7i62b_gwkiywEJ4A). Goodbye [ego depletion](https://fourbeers.fireside.fm/31). Goodbye [priming](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029081). The upshot to finding out effects didn’t replicate is that a large collaborative correction to these textbook effects (or lack thereof) has been made. But, the rewriting of  textbook effects has only made possible by the acknowledgement of the issues facing the field.

Not all replications are equal. Sometimes it’s not really that interesting to replicate every miniscule detail of a study. Nor would it be that insightful either. Is there any point in using the same sample size and sample demographics as a previous study? Especially, when the previous sample was underpowered and more homogenized than the milk in your fridge. We expect certain effects to conceptually replicate. The hypotheses we test are typically not overly specific, e.g., we expect to see a Cohen’s d of exactly 0.64 in a sample of white male students recruited from the business school at “insert generic university name” etc. We test less restrictive and more conceptual hypotheses. Both are useful, but it seems to me that a conceptual replication gives more verisimilitude to a theory/hypothesis.

<h2>Can’t catch a break.</h2>

No one likes having salt and lemon juice rubbed onto their fresh wounds. Unfortunately for social psychology, they seem to get mixed up in crisis after crisis. As the replication crisis becomes cold, the [generalizability crisis](https://psyarxiv.com/jqw35) has just been put on the stove to start heating up. The Yarkoni Generalizability Crisis paper got a metric shit tonne of attention among psychologists on Twitter.

It also did the rounds on the big psychology related podcasts - [Very Bad Wizards](https://verybadwizards.fireside.fm/182) and [Two Psychologists Four Beers](https://fourbeers.fireside.fm/38). Despite a quick read through the paper, I got absolutely no podcast invitations. I have to admit at this point that some of the points made in the paper go above my head. But let’s distil some (but definitely not all, this is a meaty paper) of the points down because they are important:
  1)	Effects should generalize across stimuli. For example, a Stroop effect should be consistent regardless of the font colour used.
  2)	Experimental designs use a sample of stimuli out of a population of possible stimuli. And, barely anyone has given a shit about this before this paper.
  3)	Just like we treat subjects/participants as a sample of a large population, we should do the same with stimuli. This means reporting the N of stimuli, and treating stimuli as a random effect, just like we do with participants.
  4)	Effect sizes shrivel and the intervals grow when stimuli are incorporated as a random effect.

Agree or disagree with the points made in the paper. What I like is that people are wiling to have a productive discussion about these kind of issues in science. See the Twitter thread.

{{<tweet 1197682895198072835>}}

These papers and responses are important metascience discussions for the field. But really there is absolutely **no reason they don’t generalize to neuroscience**. We want to know what effects are actually true and what aren’t. Our incentives in that regard are completely aligned.

<h2>Ringing in the neuropocalypse?</h2>

So far neuroscience hasn’t plunged into crisis. Is it because we’re better scientists than psychologists? **No**.

Although, some might think so
{{< tweet 1244961169489768449>}}

Is it because our effects are more “real” because they are “in the brain” (whatever on earth this kind of vacuous statement means)? **Absolutely not**.

There’s no reason to belief that questionable research practices and p-hacking are less frequent in neuroscience than psychology. There’s also no reason to believe our experimental design is stronger in neuroscience than in psychology. We might use cooler methods like lasers, drugs, and viruses. But we are prone to the exact same errors as other fields. We’re just as good at running [underpowered studies](https://www.ncbi.nlm.nih.gov/pubmed/23571845) as anyone else.

Part of the answer seems simple:
**You can’t find things if you don’t look**.

Neuroscience, especially animal research, is expensive. No one is paying out big grants to run large scale replication projects. Whether they do or do not replicate is an empirical question. But considering the incentive structures in neuroscience and psychology have been very similar over time (i.e., positive publication bias, lack of interest in publishing replications, disinterest in publishing the negative replication of an important effect), I don’t see why we would be in a different boat to social psychology.

I think it is more common than we are willing to publicly admit that highly-cited effects do not replicate. Walking around poster sessions, I've heard it mentioned that certain effects only replicate for certain labs. But, we never officially hear about the non-replications, just background whispers at conferences. How much resources should a lab piss away chasing the mirage of replicating X lab's effect, when everyone who's tried has never gotten it to work?

<h2>Should we declare a crisis?</h2>

Personally, I think we should believe way fewer of the effects we have published are real and generalizable. The first reason for this is that almost all animal research has focused on [male animals](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3008499/).  However, when we leave females out we are restricting our sampling to one half of the animal population. Yet, we may have the audacity to claim that our effects are generalizable. Come back to me when everything replicates in female animals. Not every effect should replicate in females because there are often good reasons to expect sex differences, due to different behaviours and physiology across males and females. But, we occlude ourselves from generalizable truths by omitting females from research.

Thankfully, things are changing; #SABV (sex as a biological variable) is trending. People care. The flagship journals are publishing papers [highlighting this exact issue](https://www.jneurosci.org/content/36/47/11817.short). On the other hand, some people don't care, and will never care. But, thankfully funding agencies are implementing change from the top down.

The next reason is the animals themselves. I am a rat researcher. There are lots of different strains of rats, each with their own little quirks and peculiarities. I **only** use one strain of rat in my research. Would my effects generalize from this sample of rats to a population of rats? I’ll be honest and say: I don’t know. It’s an empirical question, and I certainly hope so. But we know that for certain effects, they don’t [generalize across strains](https://www.ncbi.nlm.nih.gov/pubmed/32297781).
If you think not being able to generalize across strains is bad, wait until you hear about breeders! If you’re not familiar with rodent neuroscience, you either buy or breed your rats. There are companies that make a pretty penny from breeding rats and selling them to researchers at a premium. The last year and a half of my research life has been hell. Disappointment, frustration, just hell. I used to buy my rats from a breeding company in Quebec (I won't name names...). I study female sexual behaviour, which means I need a group of male studs that I can call upon to have sex with my female rats. The breeding company shut down the line of rats I used. So, I buy from the same company, but at a different location. As it turned out, around 50% of the new male rats refused to have sex.

Weird...

So, then I swapped supplier… and guess what? I got some stud males.

What in the world is happening at these breeding colonies that the motivation and ability to have sex was bred out of these males? And if that’s happening for something as rudimentary as sex, what’s happening to higher cognitive abilities etc.? This personal experience, combined with studies, make me worried about the generalizability of effects [within a strain](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3152440/) from different breeders. Adding to the above points, it looks like effects do not replicate in genetically similar animals depending on [geographical location]().

Going full circle and getting back to the Generalizability Crisis paper. The point of about the sampling of stimuli really made me think about current animal models. Yarkoni reminds us that we are sampling a set of stimuli for a large population of possible stimuli within a test. Expanding this to animal models, we often end up using one test to probe some cognitive, emotional, or behavioural capacity. Let’s use spatial memory as an example (sorry spatial memory folks, nothing personal). You design an experiment that tests the effect of lesions to some brain area on spatial memory. How do you test spatial memory? You use the Morris water maze. Okay, well you’ve tested a sample (n = 1) of the possible population of spatial memory tests. Would the effect generalize to the other tests of spatial memory? And would the same rat behave in a consistent way across the whole population of tests? This is an empirical question.

For now, we might get more realistic estimates of effects by including the tests used as random effects.

I’ve seen all the above as issues in what I do for a while now. It was only once I saw them through the lens of a generalizability crisis that I saw the commonality between them. Maybe it’s because of the Twitter bubble of people I choose to follow, but I haven’t seen the same open discussion of these issues among neuroscientists as I have among social psychologists. Unless, we address these issues we'll end up continuing with business as usual, hoping no one tries to replicate our effects, seeing the emperor without his clothes.
